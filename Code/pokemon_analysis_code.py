# -*- coding: utf-8 -*-
"""pokemon-analysis-code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ave42IGyET9IVqce3gRSw_BymYF2MBx_
"""

import numpy as np
import pandas as pd
import plotly.graph_objs as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
from IPython.display import Image

# read dataset
URL='https://raw.githubusercontent.com/tanisha03/DM-PartC/master/Code/data.csv?token=AI5VVFJNMZDSCWDVVHDKBWDAB2L3K'
poke = pd.read_csv(URL)
poke.head()

"""## Dataset Features
* **Number.** Pokémon ID in the Pokédex.
* **Name.** Name of the Pokémon.
* **Type_1.** Primary type.
* **Type_2.** Second type, in case the Pokémon has it.
* **Total.** Sum of all the base stats (Health Points, Attack, Defense, Special Attack, Special Defense, and Speed).
* **HP.** Base Health Points.
* **Attack.** Base Attack.
* **Defense.** Base Defense.
* **Sp_Atk.** Base Special Attack.
* **Sp_Def.** Base Special Defense.
* **Speed.** Base Speed.
* **Generation.** Number of the generation when the Pokémon was introduced.
* **isLegendary.** Boolean that indicates whether the Pokémon is Legendary or not.
* **Color.** Color of the Pokémon according to the Pokédex.
* **hasGender.** Boolean that indicates if the Pokémon can be classified as female or male.
* **Pr_male.** In case the Pokémon has Gender, the probability of its being male. The probability of being female is, of course, 1 minus this value.
* **Egg_Group_1.** Egg Group of the Pokémon.
* **Egg_Group_2.** Second Egg Group of the Pokémon, in case it has two.
* **hasMegaEvolution.** Boolean that indicates whether the Pokémon is able to Mega-evolve or not.
* **Height_m.** Height of the Pokémon, in meters.
* **Weight_kg.** Weight of the Pokémon, in kilograms.
* **Catch_Rate.** Catch Rate.
* **Body_Style.** Body Style of the Pokémon according to the Pokédex.
"""

poke.info()

poke.describe()

poke = poke.drop('Number', axis=1)

"""## Missing Data"""

# verify the missing data and quantify
missing = pd.DataFrame({'qtd_NaN_data':poke.isna().sum(), 
                        'perc_NaN_data':round((poke.isna().sum()*100/poke.shape[0]), 2)})
missing

"""Missing data
* 371 has no second type
* 77 has no Pr_Male
* 530 has no Egg_Group_2
"""

# Data when 'Pr_Male' is NaN
poke[poke['Pr_Male'].isna()].head(5)

# verify if all data with NaN in Pr_Male has field hasGender equals False
print('Without Pr Male:{} - Has gender False:{}'.format(
    poke['Number'][poke['Pr_Male'].isna()].count(),
    poke['Number'][poke['hasGender'] == False].count()))

# compare quantities of isLegendary pokemons with isLegendary and hasGender equals False
print('Is Legendary :{} - Has gender False:{}'.format(
    poke['Number'][poke['isLegendary'] == True].count(),
    poke['Number'][(poke['hasGender'] == False) & (poke['isLegendary'] == True)].count()))

# list of legendery with gender
print('**   Legendary pokemons with gender  **')
print()
poke[(poke['hasGender'] == True) & (poke['isLegendary'] == True)]

"""**Type_1** and **Egg_Group_2** has a lot of NaN data because many pokemons are only one type and one egg group.

10 percent are **Pr_Male** NaN because 77 pokemons has no gender and if these, 40 are Legendary of 46 Legendaries.
"""

threshold = sum(poke.Speed)/len(poke.Speed)
print('threshold = ',threshold)
poke["speed_level"] = ["high" if i > threshold else "low" for i in poke.Speed]
poke.loc[:10,["speed_level","Speed"]]

"""# **Unvariate Analysis**

To gain an insight of the distributions of the different variables as well as some relationships between them.
"""

poke.groupby('Type_1').count().plot(kind='bar')
poke.groupby('Type_2').count().plot(kind='bar')

"""The most common primary types are Water, Normal, and Grass, while the most common sec-
ondary type is Flying, because most of the times a Pokémon is able to fly the other type is considered first. We
can also see that more or less the half of the Pokémon do not have any secondary type.

"""

poke.groupby('Egg_Group_1').count().plot(kind='bar')
poke.groupby('Egg_Group_1').count().plot(kind='bar')

"""# **Relations and Dependencies between variables**"""

import matplotlib.pyplot as plt
import seaborn as sns
fig, ax =plt.subplots(1,6)
sns.set(rc={'figure.figsize':(10,40)})
sns.countplot(data['hasGender'], ax=ax[0])
sns.countplot(data['Color'], ax=ax[1])
sns.countplot(data['isLegendary'], ax=ax[2])
sns.countplot(data['hasMegaEvolution'], ax=ax[3])
sns.countplot(data['Generation'], ax=ax[4])
sns.countplot(data['Body_Style'], ax=ax[5])
fig.show()

"""## Categorical features analysis


"""

# list of categorical features
cat_feat = ['Type_1', 'Type_2', 'Color', 'isLegendary', 
              'hasGender', 'Egg_Group_1', 'Egg_Group_2', 
              'hasMegaEvolution', 'Body_Style']

# instantiate a subplot fig
fig = make_subplots(rows=5,cols=2,
                    vertical_spacing=0.09,
                    horizontal_spacing=0.075,
                    subplot_titles=cat_feat)

# add each trace generated by each feature
for enu, c in enumerate(cat_feat):
    # total values
    status_values = poke[c].value_counts().to_frame()
    # plot distribuition
    trace = go.Bar(x=status_values.index, 
                   y=status_values[c], 
                   text=status_values[c], 
                   textposition='auto',
                   name=c
                  )
    
    # calculate position
    row=int(np.ceil((enu+1)/2))
    col=(enu % 2)+1
    
    # add trace
    fig.append_trace(trace, row=row, col=col)
    fig.update_xaxes(tickangle = 45)
    fig.update_yaxes(showgrid = False,showticklabels = False)

# update layout
fig.update_layout(title_text='Distribuition of Categorical Data', 
                  height=1200, width=1100)

# to be viewed on github
#img = fig.to_image(format='jpg')
#Image(img)
# to normal plot
fig.show()

"""## Numerical features distribution


"""

# list of numerical features
num_feat = ['Total','HP','Attack','Defense','Sp_Atk','Sp_Def',
'Speed','Generation','Pr_Male','Height_m','Weight_kg','Catch_Rate']

# create a subplot
sub_fig = make_subplots(rows=int(np.ceil((len(num_feat))/3)),
                        cols=3, 
                        vertical_spacing=0.09,
                        horizontal_spacing=0.075,
                        subplot_titles=num_feat)
sub_fig.update_layout(title='Distribuition of Numerical Data', height=800, width=800)

# for each feature
for enu, i in enumerate(num_feat):
    
    # calculate position
    row=int(np.ceil((enu+1)/3))
    col=(enu % 3)+1
    
    # just a custom bin size
    if i == 'Pr_Male' or i == 'Height_m' or i == 'Generation':
        fig = ff.create_distplot(hist_data=[poke[i].dropna()], 
                                 group_labels=[i],
                                 bin_size=[.2], colors=[enu])
    else:
        fig = ff.create_distplot(hist_data=[poke[i].dropna()], 
                                 group_labels=[i],
                                 bin_size=[10], colors=[enu])
    # add each data at subplot
    for mydata in fig['data']:
        sub_fig.append_trace(mydata, row, col)
        


# to be viewed on github
#img_bytes = sub_fig.to_image(format="jpeg")
#Image(img_bytes)

# to normal plot
sub_fig.show()

"""## Correlations Analysis

"""

poke.corr()

#correlation map
f,ax = plt.subplots(figsize=(12, 12))
sns.heatmap(poke.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
plt.show()

# list of features to print
d_list = ['Pr_Male', 'Catch_Rate', 'Height_m', 'Weight_kg', 'Total']
dimensions = []

# add dimensions
for d in d_list:
    dimensions.append(dict(label=d, values=data[d]))

# create splom graph    
data = go.Splom(dimensions=dimensions, 
                showupperhalf=True,
                marker = dict(size=5, showscale=False,))

fig = go.Figure(data=data)
fig.update_layout(height=900, width= 900, title='Compare numerical features')
#Image(fig.to_image('jpg'))
fig.show()

"""# Clustering"""

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']

poke_dm.select_dtypes(include=numerics).columns

# let's print a dendogram of our data
import scipy.cluster.hierarchy as shc
from sklearn.cluster import AgglomerativeClustering

# list of features
num_feat = ['Total', 'HP', 'Attack', 'Defense', 'Sp_Atk', 'Sp_Def',
       'Speed', 'Height_m', 'Weight_kg', 'Catch_Rate',
       'Type_Bug', 'Type_Dark', 'Type_Dragon', 'Type_Electric', 'Type_Fairy',
       'Type_Fighting', 'Type_Fire', 'Type_Flying', 'Type_Ghost', 'Type_Grass',
       'Type_Ground', 'Type_Ice', 'Type_Normal', 'Type_Poison', 'Type_Psychic',
       'Type_Rock', 'Type_Steel', 'Type_Water']

# calculate the clusters with euclidean distance
cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')

# create dendogram with custom linkagefun
fig = ff.create_dendrogram(
    poke_dm[num_feat], orientation='bottom',
    linkagefun=lambda x: shc.linkage(poke_dm[num_feat], 'ward', metric='euclidean'), 
    color_threshold=2000
)
fig.update_layout(width=800, height=500)
fig.show()
# Image(fig.to_image('jpg'))

cl = cluster.fit_predict(poke_dm[num_feat])
# add cluster in dataframe
poke_dm['Cluster'] = cl

# totalizate each group
zero, um, dois, tres = [], [], [], []
for ii, i in enumerate(cl):
    if i==0:
        zero.append(poke[poke.index == ii].Name.values[0])
    if i==1:
        um.append(poke[poke.index == ii].Name.values[0])
    if i==2:
        dois.append(poke[poke.index == ii].Name.values[0])
    if i==3:
        tres.append(poke[poke.index == ii].Name.values[0])
       
print('0:{}, 1:{}, 2:{}, 3:{}'.format(len(zero), len(um), len(dois), len(tres)))
# print the smallest group 
print(zero)

# Check the behavior of groups with respect to field combinations
comb = [['Total','Catch_Rate'],
        ['Attack','Catch_Rate'],
        ['Defense','Catch_Rate'],
        ['Weight_kg','Height_m'],
        ['Attack','Defense'],
        ['Speed','HP']]

# create a figure
fig = make_subplots(rows=3,cols=2,
                    vertical_spacing=0.09,
                    horizontal_spacing=0.075,
                    subplot_titles= ['{} x {}'.format(i[0], i[1]) for i in comb]
                   )

# each combination with a subplot
for enu, fields in enumerate(comb):         
    value_x = poke_dm[fields[0]]
    value_y = poke_dm[fields[1]]
    
    row=int(np.ceil((enu+1)/2))
    col=(enu % 2)+1
    
    trace = go.Scatter(x=value_x,
                       y=value_y, 
                       mode='markers',
                       marker=dict(color=poke_dm['Cluster'], colorscale=["red", "green", "blue"])                       
                       )
    fig.append_trace(trace, row=row, col=col)
    
fig.update_layout(title='Comparison of some features with Clusters', showlegend=False, width=800, height=800)
fig.show()
#Image(fig.to_image('jpg'))

"""## Reducing dimensionality with PCA"""

from sklearn.decomposition import PCA as skPCA

# create two components PCA
s_pca = skPCA(n_components=2)
pca_components = s_pca.fit_transform(poke_dm[num_feat])
c_pca1 = pca_components[:,0]
c_pca2 = pca_components[:,1]
# add the componets to dataframe
poke_dm['pca1'] = c_pca1
poke_dm['pca2'] = c_pca2
poke_dm.head(2)

# Plot PCA with the previously groups
poke_dm_temp = poke_dm
trace = go.Scatter(x=poke_dm_temp['pca1'],
                   y=poke_dm_temp['pca2'],
                   mode='markers',
                   text=poke_dm_temp['Name'],
                   marker=dict(color=poke_dm_temp['Cluster'], colorscale=["red", "green", "blue"]))

fig = go.Figure(data=[trace])
fig.update_layout(title='PCA and clusters of all generations',width=800, height=500)
fig.show()
#Image(fig.to_image('jpg'))

"""## Modelling

"""

from sklearn.model_selection import cross_val_score # Cross validation function
from sklearn.metrics import confusion_matrix # To generate n confusion matrix
from sklearn.neighbors import KNeighborsClassifier # KNN model
from sklearn.tree import DecisionTreeClassifier # Decision Tree to Classifier

# features used in models
x_features = ['HP', 'Attack', 'Defense', 'Sp_Atk', 'Sp_Def',
            'Speed', 'Height_m', 'Weight_kg', 'Catch_Rate',
            'Type_Bug', 'Type_Dark', 'Type_Dragon', 'Type_Electric', 'Type_Fairy',
            'Type_Fighting', 'Type_Fire', 'Type_Flying', 'Type_Ghost', 'Type_Grass',
            'Type_Ground', 'Type_Ice', 'Type_Normal', 'Type_Poison', 'Type_Psychic',
            'Type_Rock', 'Type_Steel', 'Type_Water']
# feature target
y_features = ['isLegendary']

# separete dataset and targer
data_set = poke_dm[x_features]
target = poke_dm[y_features].values.ravel()

# list of models
models = [
          KNeighborsClassifier(3), 
          DecisionTreeClassifier(max_depth=5),          
         ]

# name models
models_name = ["Nearest Neighbors", "Decision Tree"]

total_scores = {}
for model, model_name in zip(models, models_name):
    np.random.seed = 42
    # K-fold k=5
    scores = cross_val_score(model, data_set, target, cv=10, scoring='accuracy')
    total_scores[model_name] = [scores, scores.mean(), scores.std()]
    # Accuracy
    print("{} -- K-Fold mean accuracy: {:0.3f} (std: {:0.3f})".format(model_name, scores.mean(), scores.std()))
    # Verify prediction of all data
    y_pred = model.fit(data_set,target).predict(data_set)
    
    # Confusion Matrix
    z = confusion_matrix(target, y_pred)
    x=['No Legendary', 'Is Legendary']
    y=['No Legendary', 'Is Legendary']
    
    # Generate annotations to graph
    annotations = []
    for n, row in enumerate(z):
        for m, val in enumerate(row):
            annotations.append(go.layout.Annotation(text=str(z[n][m]), x=x[m], y=y[n],
                                             xref='x1', yref='y1', showarrow=False))
            
    data = [go.Heatmap(x=x,y=y,z=z,                   
                   colorscale=["white", "lightblue"])] #amp blues peach

    layout = go.Layout(title='Confusion Matrix - {}'.format(model_name), 
                       xaxis={'title' : 'Predicted label'},
                       yaxis={'autorange' : 'reversed', 
                              'title' : 'True Label'})
    
    fig = go.Figure(data=data, layout=layout)
    fig['layout'].update(annotations=annotations, height=350, width=350)

    fig.show()
    #display(Image(fig.to_image('jpg')))
    print('----------------------------------------------------------------------------')

# boxplot of 10 folds of each model
# Gaussian Process has better result and RBF SVM has a more stable model
fig = go.Figure()
for enu, (model, (scores, v_mean, v_std)) in enumerate(total_scores.items()):
    fig.add_trace(go.Box(
        y=scores, 
        name=model,  
        boxmean='sd'))
fig.update_layout(title = 'Cross Validation Accuracy Comparacion')
fig.update_yaxes(title = 'Accuracy')
fig.update_xaxes(title = 'Models')
fig.show()
#display(Image(fig.to_image('jpg')))